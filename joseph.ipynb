{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joseph Development 7th Nov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gpt2-medium',\n",
       " 'gpt2-large',\n",
       " 'gpt2-xl',\n",
       " 'distilgpt2',\n",
       " 'facebook/opt-125m',\n",
       " 'facebook/opt-1.3b',\n",
       " 'facebook/opt-2.7b',\n",
       " 'facebook/opt-6.7b',\n",
       " 'facebook/opt-13b']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES\n",
    "\n",
    "OFFICIAL_MODEL_NAMES[1:10]\n",
    "model = HookedTransformer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "from src.probe_training_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/gpt-j-6B into HookedTransformer\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25950\n",
      "First\n",
      "Ġvascular\n",
      "ĠBan\n",
      "ĠBadge\n",
      "Ġhumane\n",
      "Ġmissile\n",
      "Ġapparently\n",
      "Ġmembrane\n",
      "Ġrenal\n",
      "Ġposition\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download(\"words\", quiet=True)\n",
    "\n",
    "\n",
    "def get_valid_word_idxs(list_of_strings):\n",
    "    english_words = set(w.lower() for w in words.words())\n",
    "    valid_english_word_idxs = []\n",
    "\n",
    "    for i, s in enumerate(list_of_strings):\n",
    "        # Remove leading \"Ġ\" character if present\n",
    "        cleaned_string = s.lstrip(\"Ġ\")\n",
    "        # Strip any other leading/trailing whitespace and convert to lowercase\n",
    "        cleaned_string = cleaned_string.strip().lower()\n",
    "        # Check if the word is in the set of English words\n",
    "        if cleaned_string in english_words:\n",
    "            valid_english_word_idxs.append(i)\n",
    "\n",
    "    return valid_english_word_idxs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "token_strings = list(model.tokenizer.get_vocab().keys())\n",
    "valid_words = get_valid_word_idxs(token_strings)\n",
    "print(len(valid_words))\n",
    "# randomly sample 100 words from the valid words\n",
    "for i in np.random.choice(valid_words, 10):\n",
    "    print(token_strings[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25950"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = model.tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "# filter vocab to only include valid words\n",
    "vocab = {k: v for k, v in vocab.items() if v in valid_words}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.),\n",
       " tensor(5.),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500),\n",
       " tensor(1.2500)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_weights_for_balanced_classes(torch.tensor([True,True,False,False,False,False,False,False,False,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "41\n",
      "torch.Size([128, 4096])\n",
      "tensor([ True,  True,  True,  True,  True, False, False, False, False,  True,\n",
      "        False, False, False,  True, False, False, False,  True,  True,  True,\n",
      "         True, False,  True, False,  True, False,  True, False, False, False,\n",
      "         True,  True,  True, False,  True,  True,  True,  True, False, False,\n",
      "        False, False,  True, False,  True,  True, False, False,  True, False,\n",
      "        False,  True, False,  True, False,  True, False,  True, False, False,\n",
      "        False,  True,  True, False,  True, False,  True,  True, False,  True,\n",
      "         True, False, False, False, False, False,  True,  True, False,  True,\n",
      "         True, False, False, False, False, False, False,  True, False,  True,\n",
      "        False, False,  True, False, False, False,  True, False, False,  True,\n",
      "        False,  True,  True, False,  True,  True,  True,  True,  True, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False,  True, False,  True, False, False, False],\n",
      "       device='cuda:0')\n",
      "tensor(0.4375, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch \n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    DataLoader,\n",
    "    WeightedRandomSampler,\n",
    "    random_split,\n",
    "\n",
    ")\n",
    "\n",
    "def get_classification_indices(vocab, regex_pattern):\n",
    "    # get indices of words that match the regex pattern\n",
    "    indices = [\n",
    "        v for k, v in vocab.items() if re.match(regex_pattern, k.strip().strip(\"Ġ\"))\n",
    "    ]\n",
    "    # get the indices of the words that don't match the regex pattern\n",
    "    not_indices = [\n",
    "        v for k, v in vocab.items() if not re.match(regex_pattern, k.strip().strip(\"Ġ\"))\n",
    "    ]\n",
    "    return indices, not_indices\n",
    "\n",
    "\n",
    "def get_regex_pattern(letter, criterion):\n",
    "    # Escaping the letter in case it has a special meaning in regex (e.g., '.' or '*')\n",
    "    escaped_letter = re.escape(letter)\n",
    "\n",
    "    if criterion == \"contains_any\":\n",
    "        # Match the letter anywhere in the string\n",
    "        return rf\"{escaped_letter}\"\n",
    "    elif criterion == \"starts\":\n",
    "        # Match strings that start with the letter\n",
    "        return rf\"^{escaped_letter}\"\n",
    "    elif criterion == \"ends\":\n",
    "        # Match strings that end with the letter\n",
    "        return rf\"{escaped_letter}$\"\n",
    "    elif criterion == \"contains_1\":\n",
    "        # Match strings that contain the letter exactly once\n",
    "        return rf\"^[^{escaped_letter}]*{escaped_letter}[^{escaped_letter}]*$\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown criterion: {criterion}\")\n",
    "\n",
    "\n",
    "def make_weights_for_balanced_classes(labels):\n",
    "    # Count of instances in each class\n",
    "    n_total = len(labels)\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "\n",
    "\n",
    "    # Weight for each sample\n",
    "    weight_per_class = {True: n_total / n_pos, False: n_total / n_neg}\n",
    "\n",
    "    # Assign weight to each sample\n",
    "    weights = [weight_per_class[label.item()] for label in labels]\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def filter_vocab(vocab):\n",
    "    english_words = set(w.lower() for w in words.words())\n",
    "    new_vocab = {}\n",
    "    for word, index in vocab.items():\n",
    "        clean_word = word.lstrip(\"Ġ\").strip().lower()\n",
    "        if clean_word in english_words and bool(roman_char_regex.match(clean_word)):\n",
    "            new_vocab[word] = index\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def get_letter_dataset(\n",
    "    criterion,\n",
    "    target,\n",
    "    embeddings,\n",
    "    vocab,\n",
    "    batch_size=32,\n",
    "    rebalance=False,\n",
    "    test_proportion=0.2,\n",
    "):\n",
    "    \n",
    "    \n",
    "    new_vocab = filter_vocab(vocab)\n",
    "    vocab_tokens = list(new_vocab.keys())\n",
    "    new_embeddings = embeddings[list(new_vocab.values())]\n",
    "    # original_indices = list(new_vocab.values())\n",
    "    # index_mapping = {v: i for i, (k, v) in enumerate(new_vocab.items())}\n",
    "\n",
    "    # get indices of words that start with the letter A\n",
    "    regex_pattern = get_regex_pattern(target, criterion)\n",
    "    labels = [bool(re.search(regex_pattern, k.strip().strip(\"Ġ\"))) for k in vocab_tokens]\n",
    "    labels = torch.tensor(labels, dtype=torch.bool, device=new_embeddings.device, requires_grad=False)\n",
    "    \n",
    "    dataset = TensorDataset(new_embeddings, labels)\n",
    "    \n",
    "    train_size = int((1 - test_proportion) * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # rebalance\n",
    "    if rebalance:\n",
    "        train_labels = dataset.tensors[1][train_dataset.indices]\n",
    "        train_weights = make_weights_for_balanced_classes(train_labels)\n",
    "        sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_letter_dataset(\n",
    "    criterion=\"starts\",\n",
    "    target=\"a\",\n",
    "    embeddings=model.W_E,\n",
    "    vocab=vocab,\n",
    "    batch_size=128,\n",
    "    rebalance=True,\n",
    ")\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "# give me a batch\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1])\n",
    "    # get proportions of positive and negative labels\n",
    "    print(sum(batch[1]) / len(batch[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f32a4c0c490>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training Code\n",
    "\n",
    "Runner needs, model, config args, probe type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214506-1c0cb3kl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1c0cb3kl' target=\"_blank\">drawn-donkey-93</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1c0cb3kl' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/1c0cb3kl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3153 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.99it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2634 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.49it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1750 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.15it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1385 | Precision: 0.9444 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.88it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0893 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.23it/s]\n",
      "Epoch 6 / 30 | Loss: 0.1069 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.46it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0755 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.24it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0616 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.78it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0697 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.26it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0488 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 879.80it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0469 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.87it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0284 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.28it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0344 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.39it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0187 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.60it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0555 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.65it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0310 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.85it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0107 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.58it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0502 | Precision: 0.9375 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.38it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0119 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.87it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0167 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 894.12it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0122 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.10it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0246 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.71it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0093 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.57it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0102 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.39it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0122 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.27it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0069 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.38it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0057 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.84it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0086 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.44it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0096 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.97it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0167 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▅▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>eval_metrics/recall</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▇▅▅▃▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▄▁▄▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▅█▁▃▄███████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>██████████████████████▃█▁██████▁████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.53775</td></tr><tr><td>eval_metrics/f1</td><td>0.92105</td></tr><tr><td>eval_metrics/loss</td><td>0.02177</td></tr><tr><td>eval_metrics/precision</td><td>87.5</td></tr><tr><td>eval_metrics/recall</td><td>97.22222</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.01672</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-donkey-93</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1c0cb3kl' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/1c0cb3kl</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214506-1c0cb3kl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214545-bw3wr77t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/bw3wr77t' target=\"_blank\">unique-firebrand-94</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/bw3wr77t' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/bw3wr77t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.12it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2581 | Precision: 0.8889 | Recall: 0.9412: 100%|██████████| 771/771 [00:00<00:00, 945.77it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1133 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.72it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0944 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.15it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0816 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.77it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0414 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.82it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0493 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.70it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0506 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.90it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0456 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 899.25it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0356 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 903.14it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0211 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 902.50it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0189 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.38it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0234 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.36it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0161 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.92it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0114 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 903.76it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0160 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.36it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0120 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.80it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0117 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.05it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0155 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.59it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0061 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.41it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0063 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.57it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0110 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.83it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.75it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0102 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.77it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0119 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.61it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0058 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.34it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0085 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.51it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0135 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.15it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0073 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.93it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0026 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▆▆▆▆▆▆▆▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇██▇███████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▅██▃███▅█▅█████████▅██████▅████████████</td></tr><tr><td>training_metrics/recall</td><td>█▁██████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.69183</td></tr><tr><td>eval_metrics/f1</td><td>0.92593</td></tr><tr><td>eval_metrics/loss</td><td>0.01167</td></tr><tr><td>eval_metrics/precision</td><td>86.2069</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00261</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-firebrand-94</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/bw3wr77t' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/bw3wr77t</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214545-bw3wr77t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214623-i7jirqft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/i7jirqft' target=\"_blank\">comic-planet-95</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/i7jirqft' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/i7jirqft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.4117 | Precision: 0.7500 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 897.29it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2547 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.38it/s]\n",
      "Epoch 3 / 30 | Loss: 0.2290 | Precision: 0.9333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.82it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1144 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.92it/s]\n",
      "Epoch 5 / 30 | Loss: 0.1083 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.32it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0921 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.88it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0551 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.55it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0536 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.56it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0485 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.81it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0326 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.98it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0431 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 896.71it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0231 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.25it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0231 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.77it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0330 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 886.03it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0153 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 908.96it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0089 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 899.18it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0127 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.09it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0146 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.16it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0145 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.58it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0188 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.80it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0130 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 890.25it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0102 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 889.29it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 889.53it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0149 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 892.08it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0081 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 892.16it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0132 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 892.53it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0079 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 872.83it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0083 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 891.37it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0053 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 890.83it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▆▆▇▇▇▇▇▇████████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▅▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▄▅▅▆▆▆▇▆▇▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▇▄▃▃▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▅▁▆█████████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.97674</td></tr><tr><td>eval_metrics/loss</td><td>0.01309</td></tr><tr><td>eval_metrics/precision</td><td>95.45455</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.005</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-planet-95</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/i7jirqft' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/i7jirqft</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214623-i7jirqft/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214701-a52u85lg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/a52u85lg' target=\"_blank\">volcanic-snow-96</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/a52u85lg' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/a52u85lg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3453 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.51it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1731 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.67it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1414 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.48it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0916 | Precision: 0.9444 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.19it/s]\n",
      "Epoch 5 / 30 | Loss: 0.1001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.05it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0501 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.74it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0629 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.05it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0457 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.48it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0348 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.04it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0403 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.59it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0217 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.39it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0087 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.32it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0179 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.30it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0138 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.71it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0183 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.24it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0101 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.05it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0143 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.47it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0098 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.94it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0084 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.66it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0119 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.39it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0120 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.11it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.18it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0048 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.36it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.02it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0092 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.20it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0043 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.98it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.33it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0032 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.98it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0172 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.54it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0043 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▄▄▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▄▃▅▅▆▇▆▇▇▇▇▇▇███████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▃▃▅▅▆▆▆▇▆▆▇▇▇▇██████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▄▃▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁████████▁██████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▃▁██▂███████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.96296</td></tr><tr><td>eval_metrics/loss</td><td>0.00936</td></tr><tr><td>eval_metrics/precision</td><td>92.85714</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00435</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-snow-96</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/a52u85lg' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/a52u85lg</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214701-a52u85lg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214737-cm8ycdjs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/cm8ycdjs' target=\"_blank\">upbeat-glitter-97</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/cm8ycdjs' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/cm8ycdjs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3595 | Precision: 0.8333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.44it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2088 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.22it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1307 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.69it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.58it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0806 | Precision: 0.9333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.24it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0605 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.66it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0460 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.87it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0670 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.14it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0282 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.39it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0396 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.24it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0489 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.03it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0304 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.04it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0235 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.80it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0318 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.63it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0137 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 955.15it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0164 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.91it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0178 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.64it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0060 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.58it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0047 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.93it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0080 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.68it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0073 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.55it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0056 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.69it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0066 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.67it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.54it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.56it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0029 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.24it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.72it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0047 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.88it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0078 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.76it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇██▇█▇▇██████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▄▅▅▅▆▇▇▇▇▇▇▇▇▇█▇██▇█▇▇██████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▃▄▄▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>eval_metrics/recall</td><td>███████████████▁█▁██▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>███████████▁████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.92296</td></tr><tr><td>eval_metrics/f1</td><td>0.97778</td></tr><tr><td>eval_metrics/loss</td><td>0.00786</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>95.65217</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00412</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-glitter-97</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/cm8ycdjs' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/cm8ycdjs</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214737-cm8ycdjs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214813-1e96g7ni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1e96g7ni' target=\"_blank\">lilac-disco-98</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1e96g7ni' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/1e96g7ni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3367 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.12it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1692 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.14it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1328 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.20it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1105 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.25it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0689 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.25it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0706 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.16it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0703 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.92it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0370 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.70it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0354 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.63it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0162 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 892.98it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0234 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 905.27it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0240 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.77it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0110 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.69it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0243 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.42it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0189 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.67it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0109 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.27it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0076 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.43it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0070 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.42it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0093 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.28it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0085 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.52it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0040 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.34it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0087 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.42it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.86it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.06it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0078 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.36it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0028 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.72it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0056 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.41it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0101 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 908.77it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.12it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0039 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▄▅▆▆▆▆▇▇▇███████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▃▄▅▅▅▆▆▇▇█▇█████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▃▃▄▄▄▅▅▆▆▇▇█████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁███████████████████▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▄▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>███▅▁███████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.92296</td></tr><tr><td>eval_metrics/f1</td><td>0.97778</td></tr><tr><td>eval_metrics/loss</td><td>0.00989</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>95.65217</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00389</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-disco-98</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/1e96g7ni' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/1e96g7ni</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214813-1e96g7ni/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214851-7ktfmhic</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/7ktfmhic' target=\"_blank\">wobbly-bee-99</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/7ktfmhic' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/7ktfmhic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2361 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.01it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1687 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.27it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0918 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 903.40it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1150 | Precision: 0.9474 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.19it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0581 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.18it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0420 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 885.49it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0228 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.62it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0159 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.57it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0139 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.67it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0105 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.23it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0147 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.08it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0088 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.26it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0136 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.19it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.83it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0039 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.44it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0069 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.30it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.40it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0096 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.05it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.44it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.62it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.14it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.22it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0038 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.03it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0029 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.36it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0019 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.64it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.26it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.31it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0071 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.35it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.92it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0026 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▅▅▆▅▅▅▅▆▆▆▆▆▆▆▇▇▇██████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▄▄▅▅▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>eval_metrics/recall</td><td>▁▅████▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▅▅██████████████████████▅██████████████</td></tr><tr><td>training_metrics/recall</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.76888</td></tr><tr><td>eval_metrics/f1</td><td>0.92683</td></tr><tr><td>eval_metrics/loss</td><td>0.00816</td></tr><tr><td>eval_metrics/precision</td><td>90.47619</td></tr><tr><td>eval_metrics/recall</td><td>95.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00258</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-bee-99</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/7ktfmhic' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/7ktfmhic</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214851-7ktfmhic/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_214926-zwykrozz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/zwykrozz' target=\"_blank\">silvery-cloud-100</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/zwykrozz' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/zwykrozz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3676 | Precision: 0.8333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.42it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1684 | Precision: 1.0000 | Recall: 0.9231: 100%|██████████| 771/771 [00:00<00:00, 938.90it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1306 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.75it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1490 | Precision: 0.9444 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.47it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0885 | Precision: 0.9500 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.12it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0435 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 890.28it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0295 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.33it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0517 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.61it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0211 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.33it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0152 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 895.76it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0209 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.71it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0227 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.00it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0101 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.22it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0199 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.34it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0140 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.22it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0067 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.72it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0068 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.03it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0053 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.33it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0072 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.14it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0051 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 908.67it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0145 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.40it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0029 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.01it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0093 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.45it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0040 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.77it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0032 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.06it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0017 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.20it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0057 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.63it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.26it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 900.75it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0052 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▃▄▄▄▄▅▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▅▄▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>██▁█████████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.92857</td></tr><tr><td>eval_metrics/loss</td><td>0.01006</td></tr><tr><td>eval_metrics/precision</td><td>86.66667</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00517</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-cloud-100</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/zwykrozz' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/zwykrozz</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_214926-zwykrozz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215002-sliypfho</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/sliypfho' target=\"_blank\">toasty-waterfall-101</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/sliypfho' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/sliypfho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3548 | Precision: 0.8571 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.86it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2146 | Precision: 0.9474 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.09it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1360 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.45it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1060 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.07it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0725 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.00it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0409 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.91it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0338 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.72it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0393 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.71it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0116 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.33it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0105 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.95it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0600 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.60it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0114 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 980.95it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0146 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.68it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0101 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.30it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0192 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.95it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0040 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.99it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0038 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.40it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0056 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.04it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0083 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 898.49it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0065 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.02it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.46it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0092 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.15it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0103 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.89it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.25it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0057 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.84it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0045 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.85it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0027 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.61it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0067 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.85it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0209 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.65it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0027 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▃▃▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▂▂▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▂▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▃▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁█▄███████████▃█████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>100.0</td></tr><tr><td>eval_metrics/f1</td><td>1.0</td></tr><tr><td>eval_metrics/loss</td><td>0.00897</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00272</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-waterfall-101</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/sliypfho' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/sliypfho</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215002-sliypfho/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215038-csff3q1r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/csff3q1r' target=\"_blank\">radiant-dream-102</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/csff3q1r' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/csff3q1r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.1717 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 887.74it/s]\n",
      "Epoch 2 / 30 | Loss: 0.0877 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 889.05it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0413 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 893.10it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0349 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 885.43it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0210 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 884.80it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0143 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 877.86it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0136 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 893.59it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0086 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 901.30it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0095 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.56it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.98it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0067 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 877.67it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0107 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.79it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0098 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.83it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0090 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.59it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0036 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.44it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0059 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.76it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0061 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 902.01it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.70it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0026 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.91it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0016 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.63it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.20it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0017 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.60it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0067 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.79it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.58it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.88it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0017 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.19it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0016 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.44it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0157 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.92it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.85it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0066 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▆▇██████████████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▆███████████████▇▇█████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>eval_metrics/recall</td><td>███████████████████▁▁█████████</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▂▂▄▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>█▃███▁▂███████████▃█████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.61479</td></tr><tr><td>eval_metrics/f1</td><td>0.81481</td></tr><tr><td>eval_metrics/loss</td><td>0.00779</td></tr><tr><td>eval_metrics/precision</td><td>68.75</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00659</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-dream-102</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/csff3q1r' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/csff3q1r</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215038-csff3q1r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215116-rz6z7aav</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/rz6z7aav' target=\"_blank\">clear-wave-103</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/rz6z7aav' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/rz6z7aav</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2517 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.44it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1191 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 982.06it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0508 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 975.68it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0297 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.15it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0230 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.44it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0160 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.05it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0141 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.57it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0080 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 980.33it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.83it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0075 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.79it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0032 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.49it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0096 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.81it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.56it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.59it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.12it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.40it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.70it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0010 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.35it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0379 | Precision: 0.9474 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 975.78it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 975.53it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0012 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.69it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.07it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 988.06it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0010 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.77it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.76it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.39it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.99it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0074 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.13it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0010 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.85it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0011 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▅▅▅▅▆▇▇▇▇███████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▄▄▄▄▅▇▇▇▇███████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▃▃▄▄▅▇▇▇▇███████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁█▁██████▂██████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>100.0</td></tr><tr><td>eval_metrics/f1</td><td>1.0</td></tr><tr><td>eval_metrics/loss</td><td>0.00213</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00107</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-wave-103</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/rz6z7aav' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/rz6z7aav</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215116-rz6z7aav/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215152-xprqs639</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/xprqs639' target=\"_blank\">lunar-snowflake-104</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/xprqs639' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/xprqs639</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3427 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 894.63it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1748 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.67it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1552 | Precision: 0.9524 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.92it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1335 | Precision: 0.9500 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.14it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0877 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.74it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0618 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.04it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0400 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.25it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0826 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.13it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0239 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.84it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0268 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.83it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0246 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.63it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0245 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.48it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0086 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.32it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0107 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.90it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0087 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.01it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0139 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.34it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0086 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.84it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0166 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.14it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0064 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.34it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0057 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.49it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.19it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0109 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.96it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0073 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.62it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.43it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0059 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.75it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.56it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0134 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.91it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0095 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.40it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.69it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▄▆▆▇▇▇▇▇███▇▇███████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇█████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▃▄▅▆▆▆▆▆▇▇▇▆▆▇▇▇█▇▇█████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁███▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁███████████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.69183</td></tr><tr><td>eval_metrics/f1</td><td>0.88235</td></tr><tr><td>eval_metrics/loss</td><td>0.00955</td></tr><tr><td>eval_metrics/precision</td><td>83.33333</td></tr><tr><td>eval_metrics/recall</td><td>93.75</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00224</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-snowflake-104</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/xprqs639' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/xprqs639</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215152-xprqs639/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215229-q5k854ep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/q5k854ep' target=\"_blank\">lunar-bush-105</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/q5k854ep' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/q5k854ep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3442 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.19it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2523 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.96it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1494 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.14it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0962 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.01it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0763 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.43it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0831 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.92it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0663 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 902.46it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0298 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.31it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0478 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.30it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0294 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 955.22it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0410 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.86it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0189 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.86it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0224 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.99it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0191 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.43it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0350 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.24it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0150 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.65it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0138 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.52it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0082 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.35it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0090 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.13it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0152 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.21it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0079 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.06it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0156 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.49it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0068 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.79it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0185 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.99it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0053 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.69it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0065 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.95it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0048 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.46it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0228 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.73it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.65it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 910.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▄▅▅▆▆▆▆▆▆▆▇▇████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▄▄▅▅▅▅▅▆▆▆▇▇▇▇█▇████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▄▃▃▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▃███▁▅█▅█▅▂█████▆███████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▄██▁████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.96667</td></tr><tr><td>eval_metrics/loss</td><td>0.01131</td></tr><tr><td>eval_metrics/precision</td><td>93.54839</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00251</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-bush-105</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/q5k854ep' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/q5k854ep</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215229-q5k854ep/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215305-yhfvwe58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/yhfvwe58' target=\"_blank\">graceful-glitter-106</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/yhfvwe58' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/yhfvwe58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2994 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.91it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1420 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.97it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0742 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.06it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0706 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.70it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0575 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.47it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0257 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.07it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0290 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.85it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0195 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.57it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0191 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.69it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0091 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.38it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0162 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.74it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0064 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.08it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0103 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.69it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0082 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.72it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0078 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.74it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0083 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.48it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.41it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0027 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.13it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0051 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.57it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0065 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.68it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.28it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0028 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 984.43it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0036 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.12it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 955.60it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.75it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0080 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.99it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.75it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.90it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.53it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0034 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▄▅▆▆▆▇▇▇▇▇██████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▃▄▅▅▅▆▆▆▆▇▇▇▇▇██████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇██████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁██████████████▃████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.92308</td></tr><tr><td>eval_metrics/loss</td><td>0.00687</td></tr><tr><td>eval_metrics/precision</td><td>85.71429</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00342</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-glitter-106</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/yhfvwe58' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/yhfvwe58</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215305-yhfvwe58/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215341-scv8am0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/scv8am0u' target=\"_blank\">dandy-water-107</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/scv8am0u' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/scv8am0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2655 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.50it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1495 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.66it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0954 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.28it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0767 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.65it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0488 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.72it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0368 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.00it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0230 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.92it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0188 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.31it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0177 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.03it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0278 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.00it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0108 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.69it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0136 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.05it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0122 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.10it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0066 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.73it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0243 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.01it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0060 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.26it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0077 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.64it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0056 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.41it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0045 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.17it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.49it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0057 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.82it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0030 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.45it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0043 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.48it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0028 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.90it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.54it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.81it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0029 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.65it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0030 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.64it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.87it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▅▅▅▆▆▆▇▇▇▇██████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▂▃▃▃▄▄▅▆▇▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▂▃▃▃▃▄▅▆▆▆▇▇████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>█▁▂████▁████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>100.0</td></tr><tr><td>eval_metrics/f1</td><td>1.0</td></tr><tr><td>eval_metrics/loss</td><td>0.00571</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00348</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-water-107</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/scv8am0u' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/scv8am0u</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215341-scv8am0u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215417-09pcsbnn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/09pcsbnn' target=\"_blank\">eager-pine-108</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/09pcsbnn' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/09pcsbnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.4466 | Precision: 0.6667 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.17it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2178 | Precision: 0.9286 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.63it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1559 | Precision: 0.9444 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.51it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0856 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.39it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0741 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.98it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0937 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 991.40it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0599 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.44it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0386 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.11it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0339 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 955.35it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0388 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.37it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0324 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.26it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0440 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.44it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0226 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.94it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0158 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.00it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0147 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.35it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0296 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.96it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0094 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 983.63it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0084 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.08it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0084 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.15it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0062 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.12it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0049 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 905.26it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0073 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.71it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0084 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.52it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0066 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.03it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0190 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.52it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0049 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.29it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0045 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.80it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0098 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.03it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0065 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.75it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▃▄▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▂▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▂▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇██</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▇▄▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▄▆▆███▄██▅█████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>█▁██████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.84592</td></tr><tr><td>eval_metrics/f1</td><td>0.94444</td></tr><tr><td>eval_metrics/loss</td><td>0.01202</td></tr><tr><td>eval_metrics/precision</td><td>89.47368</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00417</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-pine-108</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/09pcsbnn' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/09pcsbnn</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215417-09pcsbnn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215453-37cj2x6m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/37cj2x6m' target=\"_blank\">electric-bird-109</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/37cj2x6m' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/37cj2x6m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.1174 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.35it/s]\n",
      "Epoch 2 / 30 | Loss: 0.0509 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.67it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0205 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.37it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0129 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.04it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0078 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.96it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0070 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.16it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0052 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.34it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.06it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0027 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.49it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.57it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0019 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.35it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 985.92it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.93it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.06it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0014 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.14it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0009 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.17it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0008 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.01it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0012 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.01it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0009 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.13it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.93it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.27it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.16it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.31it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.78it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.95it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.95it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.65it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.61it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.24it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▁▁▁▁▁▁▁██████████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▁▁▁▁▁▁▁██████████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▁▁▁▁▁▁▁██████████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▄▃▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▃██████▁▂███████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.92296</td></tr><tr><td>eval_metrics/f1</td><td>0.8</td></tr><tr><td>eval_metrics/loss</td><td>0.00426</td></tr><tr><td>eval_metrics/precision</td><td>66.66667</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00045</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-bird-109</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/37cj2x6m' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/37cj2x6m</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215453-37cj2x6m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215529-lui6vlqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/lui6vlqh' target=\"_blank\">amber-aardvark-110</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/lui6vlqh' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/lui6vlqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3445 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.58it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1894 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.20it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1289 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.20it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0761 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.87it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0658 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.87it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0665 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 892.35it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0268 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.03it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0343 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.35it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0232 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.17it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0368 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.20it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0175 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.01it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0239 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 900.59it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0230 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.48it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0118 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 903.95it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0130 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.11it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0111 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.94it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0153 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.01it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0095 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.81it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0078 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.66it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0068 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.70it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0131 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.13it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0087 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.85it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0048 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.35it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0046 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.98it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.00it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0062 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.70it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0097 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.12it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.88it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0029 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.56it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▇▇▇▇▇███████████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▆▆▆▆▇▇██████████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>eval_metrics/recall</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▇▅▃▃▃▃▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▅▅█▅▅██████████▅███████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.69183</td></tr><tr><td>eval_metrics/f1</td><td>0.93103</td></tr><tr><td>eval_metrics/loss</td><td>0.01232</td></tr><tr><td>eval_metrics/precision</td><td>90.0</td></tr><tr><td>eval_metrics/recall</td><td>96.42857</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00247</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-aardvark-110</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/lui6vlqh' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/lui6vlqh</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215529-lui6vlqh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215603-ync86m64</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ync86m64' target=\"_blank\">lucky-meadow-111</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ync86m64' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/ync86m64</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.4879 | Precision: 0.6250 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.91it/s]\n",
      "Epoch 2 / 30 | Loss: 0.2619 | Precision: 0.9286 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.43it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1818 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.86it/s]\n",
      "Epoch 4 / 30 | Loss: 0.1521 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.85it/s]\n",
      "Epoch 5 / 30 | Loss: 0.1190 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.27it/s]\n",
      "Epoch 6 / 30 | Loss: 0.1345 | Precision: 0.8750 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.70it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0732 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.64it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0722 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.29it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0647 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.58it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0763 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.21it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0242 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.54it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0423 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.95it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0336 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.00it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0280 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.01it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0201 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.21it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0227 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.03it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0393 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.80it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0434 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.60it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0107 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.36it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0329 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.34it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0114 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.12it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0100 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.50it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0145 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.75it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0175 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.36it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0102 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.43it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0156 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.22it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0082 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 933.75it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0084 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.59it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0070 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 947.65it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0100 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▅▆▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▄▅▆▆▆▆▇▇▇▇▇█████████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▅▅▅▅▆▇▆▇▇▇█████████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▆▆▆▆▆▆▆▆█▆▆▆▆████████████████</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▇▅▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▃▆█████▄█▆███████████████████████▆█████</td></tr><tr><td>training_metrics/recall</td><td>██▁█████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.61479</td></tr><tr><td>eval_metrics/f1</td><td>0.95652</td></tr><tr><td>eval_metrics/loss</td><td>0.02171</td></tr><tr><td>eval_metrics/precision</td><td>91.66667</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00998</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-meadow-111</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ync86m64' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/ync86m64</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215603-ync86m64/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215638-k1xwun58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/k1xwun58' target=\"_blank\">exalted-jazz-112</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/k1xwun58' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/k1xwun58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.3044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.57it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1970 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 934.91it/s]\n",
      "Epoch 3 / 30 | Loss: 0.1174 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.67it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0859 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.46it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0890 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.49it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0648 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.31it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0423 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.45it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0484 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.06it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0465 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.61it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0215 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.70it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0367 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.63it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0180 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.34it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0555 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.83it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0161 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.33it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0062 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.76it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0240 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.35it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0228 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.54it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0148 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.58it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0065 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.36it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0189 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.44it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0083 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.41it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0230 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.60it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0099 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.50it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0049 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.93it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.65it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.05it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0059 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 982.33it/s] \n",
      "Epoch 28 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.58it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0051 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.93it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▆▇▆▆▇███████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▂▃▄▄▄▄▅▅▅▆▆▆▆▆▇█▇▇███████████</td></tr><tr><td>eval_metrics/loss</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▃▃▃▃▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇</td></tr><tr><td>eval_metrics/recall</td><td>████████████████████████████▁█</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▄▃▃▂▂▃▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▅█▆███▄█████▆███████▆█████▅████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.61479</td></tr><tr><td>eval_metrics/f1</td><td>0.92754</td></tr><tr><td>eval_metrics/loss</td><td>0.0141</td></tr><tr><td>eval_metrics/precision</td><td>86.48649</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00351</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-jazz-112</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/k1xwun58' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/k1xwun58</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215638-k1xwun58/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215713-eecd5zb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/eecd5zb5' target=\"_blank\">clean-deluge-113</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/eecd5zb5' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/eecd5zb5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2432 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.50it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1228 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.79it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0977 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.55it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0437 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.46it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0353 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 982.66it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0451 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.06it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0428 | Precision: 0.9333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.94it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0150 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.29it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0089 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.75it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0159 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.97it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0042 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.73it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0132 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.27it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0044 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 975.02it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.32it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0080 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 981.10it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 958.07it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0039 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 953.21it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0222 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.90it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 961.62it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0021 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.26it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0026 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.09it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0069 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.10it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.52it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.47it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0050 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.66it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.09it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.52it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0049 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 973.21it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0030 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.03it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▁▂▂▂▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇█▇▇▇▇▇▇▇</td></tr><tr><td>eval_metrics/f1</td><td>▁▁▂▂▂▄▄▅▅▅▅▅▅▅▄▄▄▄▄▄▄▆█▆▆▆▆▆▆▆</td></tr><tr><td>eval_metrics/loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▁▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆█▆▆▆▆▆▆▆</td></tr><tr><td>eval_metrics/recall</td><td>██████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁██▃████████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.76888</td></tr><tr><td>eval_metrics/f1</td><td>0.84211</td></tr><tr><td>eval_metrics/loss</td><td>0.00486</td></tr><tr><td>eval_metrics/precision</td><td>80.0</td></tr><tr><td>eval_metrics/recall</td><td>88.88889</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00181</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-deluge-113</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/eecd5zb5' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/eecd5zb5</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215713-eecd5zb5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215748-9jl654s3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/9jl654s3' target=\"_blank\">peach-snow-114</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/9jl654s3' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/9jl654s3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2264 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.06it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1211 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 911.30it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0785 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.59it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0320 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.75it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0308 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.27it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0516 | Precision: 0.9412 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.17it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0251 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.01it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0182 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.35it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0099 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 913.50it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0096 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.33it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0095 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.78it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0068 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.58it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0096 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 970.87it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0036 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.63it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.65it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 949.06it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0055 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.43it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 918.30it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.89it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0054 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.63it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0028 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.79it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0018 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.17it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.49it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.78it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0019 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 931.62it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0014 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.72it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0012 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.95it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0021 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.63it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.05it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0020 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▆▆▆▆▆▆▆▇▇▇▇█████████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███</td></tr><tr><td>eval_metrics/loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▃▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇█▇▇███</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>████▁███████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>100.0</td></tr><tr><td>eval_metrics/f1</td><td>1.0</td></tr><tr><td>eval_metrics/loss</td><td>0.00402</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00197</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-snow-114</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/9jl654s3' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/9jl654s3</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215748-9jl654s3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215823-ne48moiy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ne48moiy' target=\"_blank\">clean-durian-115</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ne48moiy' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/ne48moiy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2675 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 916.42it/s]\n",
      "Epoch 2 / 30 | Loss: 0.1898 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.30it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0859 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.74it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0711 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.19it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0708 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.39it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0475 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.11it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0315 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.05it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0509 | Precision: 0.9444 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.70it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0805 | Precision: 0.9286 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.69it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0183 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.20it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0280 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.99it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0235 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 926.76it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0112 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 909.47it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0090 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.74it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0103 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.73it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0077 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 939.15it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0089 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.64it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0070 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.04it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0210 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 912.36it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0058 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.02it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0051 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.71it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0076 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.81it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.49it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.35it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0103 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 922.72it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0038 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 952.93it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0024 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.19it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0030 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 903.95it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0049 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 945.27it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0023 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 935.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▄▄▆▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>eval_metrics/f1</td><td>▁▃▃▅▅▅▆▆▆▆▇▇▇▇▇███████████████</td></tr><tr><td>eval_metrics/loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▂▃▄▅▅▅▆▆▆▇▆▇▇▇█▇█████████████</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▆▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▄▆██▆█████▆████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.76888</td></tr><tr><td>eval_metrics/f1</td><td>0.91429</td></tr><tr><td>eval_metrics/loss</td><td>0.01043</td></tr><tr><td>eval_metrics/precision</td><td>84.21053</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00232</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-durian-115</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/ne48moiy' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/ne48moiy</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215823-ne48moiy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215900-qv71ffwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/qv71ffwf' target=\"_blank\">pious-durian-116</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/qv71ffwf' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/qv71ffwf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.0529 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.31it/s]\n",
      "Epoch 2 / 30 | Loss: 0.0169 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 907.77it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0079 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.99it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0037 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 904.41it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0033 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.59it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0021 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 925.89it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0009 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.54it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0011 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 915.08it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 906.90it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.72it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.12it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0003 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.54it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0002 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.97it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0002 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.78it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.71it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.90it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.48it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 982.83it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 941.14it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 957.92it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.53it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.52it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 955.99it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.70it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 923.23it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 954.10it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.69it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 919.48it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.46it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0001 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>100.0</td></tr><tr><td>eval_metrics/f1</td><td>1.0</td></tr><tr><td>eval_metrics/loss</td><td>0.00014</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>100.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.0001</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-durian-116</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/qv71ffwf' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/qv71ffwf</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215900-qv71ffwf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_215935-tzab5h5k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/tzab5h5k' target=\"_blank\">wobbly-sunset-117</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/tzab5h5k' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/tzab5h5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.2271 | Precision: 0.8333 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 928.69it/s]\n",
      "Epoch 2 / 30 | Loss: 0.0571 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 932.42it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0410 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.42it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0298 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.05it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0134 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.88it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0095 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 960.07it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0058 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.89it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0035 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.44it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0036 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 979.05it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0039 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 965.79it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0019 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 944.55it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0022 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.71it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0019 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 972.98it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0032 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.23it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0014 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.36it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0015 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 963.27it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0014 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.44it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.01it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0008 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 974.00it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0011 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 979.41it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0008 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.87it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 971.29it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0009 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 976.60it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0016 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 976.63it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.01it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.82it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.82it/s]\n",
      "Epoch 28 / 30 | Loss: 0.0041 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 927.27it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 936.82it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0011 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>▁▅▆▆▆▆▆▆██▆▆█▆▆▆▆▆▆▆▆▆▆▆▆█████</td></tr><tr><td>eval_metrics/f1</td><td>▁▄▆▆▆▆▆▆██▃▃█▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅</td></tr><tr><td>eval_metrics/loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▄▆▆▆▆▆▆██▅▅█▅▅▅▅▅▅▅▅▅▅▅▅█████</td></tr><tr><td>eval_metrics/recall</td><td>██████████▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁███▂███████████████████████████████████</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.69183</td></tr><tr><td>eval_metrics/f1</td><td>0.6</td></tr><tr><td>eval_metrics/loss</td><td>0.00667</td></tr><tr><td>eval_metrics/precision</td><td>50.0</td></tr><tr><td>eval_metrics/recall</td><td>75.0</td></tr><tr><td>metrics/learning_rate</td><td>0.001</td></tr><tr><td>training_metrics/loss</td><td>0.00106</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sunset-117</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/tzab5h5k' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/tzab5h5k</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_215935-tzab5h5k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_220010-klp5cozd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/klp5cozd' target=\"_blank\">jumping-shadow-118</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/klp5cozd' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/klp5cozd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 30 | Loss: 0.0833 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 921.97it/s]\n",
      "Epoch 2 / 30 | Loss: 0.0340 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.36it/s]\n",
      "Epoch 3 / 30 | Loss: 0.0148 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 930.83it/s]\n",
      "Epoch 4 / 30 | Loss: 0.0111 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 929.71it/s]\n",
      "Epoch 5 / 30 | Loss: 0.0064 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 937.86it/s]\n",
      "Epoch 6 / 30 | Loss: 0.0031 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 920.11it/s]\n",
      "Epoch 7 / 30 | Loss: 0.0025 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 917.20it/s]\n",
      "Epoch 8 / 30 | Loss: 0.0026 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 914.00it/s]\n",
      "Epoch 9 / 30 | Loss: 0.0012 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 924.81it/s]\n",
      "Epoch 10 / 30 | Loss: 0.0013 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.09it/s]\n",
      "Epoch 11 / 30 | Loss: 0.0010 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.37it/s]\n",
      "Epoch 12 / 30 | Loss: 0.0008 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 964.08it/s]\n",
      "Epoch 13 / 30 | Loss: 0.0008 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 938.78it/s]\n",
      "Epoch 14 / 30 | Loss: 0.0007 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.55it/s]\n",
      "Epoch 15 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 959.99it/s]\n",
      "Epoch 16 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 948.57it/s]\n",
      "Epoch 17 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 967.24it/s]\n",
      "Epoch 18 / 30 | Loss: 0.0005 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 950.50it/s]\n",
      "Epoch 19 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 969.64it/s]\n",
      "Epoch 20 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 940.50it/s]\n",
      "Epoch 21 / 30 | Loss: 0.0006 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 951.90it/s]\n",
      "Epoch 22 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 978.33it/s]\n",
      "Epoch 23 / 30 | Loss: 0.0003 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 942.27it/s]\n",
      "Epoch 24 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 956.12it/s]\n",
      "Epoch 25 / 30 | Loss: 0.0002 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 966.83it/s]\n",
      "Epoch 26 / 30 | Loss: 0.0003 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 979.39it/s]\n",
      "Epoch 27 / 30 | Loss: 0.0003 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 968.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 943.98it/s]\n",
      "Epoch 29 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 946.34it/s]\n",
      "Epoch 30 / 30 | Loss: 0.0004 | Precision: 1.0000 | Recall: 1.0000: 100%|██████████| 771/771 [00:00<00:00, 962.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/f1</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_metrics/recall</td><td>██████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/learning_rate</td><td>████████████████████████████████████▁▁▁▁</td></tr><tr><td>training_metrics/loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_metrics/recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_metrics/accuracy</td><td>99.92296</td></tr><tr><td>eval_metrics/f1</td><td>0.66667</td></tr><tr><td>eval_metrics/loss</td><td>0.00148</td></tr><tr><td>eval_metrics/precision</td><td>100.0</td></tr><tr><td>eval_metrics/recall</td><td>50.0</td></tr><tr><td>metrics/learning_rate</td><td>0.0001</td></tr><tr><td>training_metrics/loss</td><td>0.00035</td></tr><tr><td>training_metrics/precision</td><td>1.0</td></tr><tr><td>training_metrics/recall</td><td>1.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-shadow-118</strong> at: <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/klp5cozd' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/klp5cozd</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_220010-klp5cozd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/github_repositories/alphabetical_probe/wandb/run-20231107_220045-k2l7hf74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/letter_presence_probes/runs/k2l7hf74' target=\"_blank\">aggregate_artifact_logging</a></strong> to <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/letter_presence_probes' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/letter_presence_probes/runs/k2l7hf74' target=\"_blank\">https://wandb.ai/jbloom/letter_presence_probes/runs/k2l7hf74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Path is not a file: probe_A_starts.pt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=254'>255</a>\u001b[0m         wandb\u001b[39m.\u001b[39mfinish()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=256'>257</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model, \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=258'>259</a>\u001b[0m probe_weights_tensor \u001b[39m=\u001b[39m all_probe_training_runner(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=259'>260</a>\u001b[0m     embeddings\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mW_E\u001b[39m.\u001b[39;49mdetach(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=260'>261</a>\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=261'>262</a>\u001b[0m     \u001b[39m# alphabet=\"A\",\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=262'>263</a>\u001b[0m     criteria_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstarts\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=263'>264</a>\u001b[0m     probe_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=264'>265</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=265'>266</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=266'>267</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=267'>268</a>\u001b[0m     train_test_split\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=268'>269</a>\u001b[0m     rebalance\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=269'>270</a>\u001b[0m     use_wandb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=270'>271</a>\u001b[0m )\n",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m letter, probe \u001b[39min\u001b[39;00m probes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     artifact \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mArtifact(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m         letter,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m         \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_tensors\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m         description\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLetter presence probe weights tensor for letter \u001b[39m\u001b[39m{\u001b[39;00mletter\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m     )\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     artifact\u001b[39m.\u001b[39;49madd_file(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprobe_\u001b[39;49m\u001b[39m{\u001b[39;49;00mletter\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mcriteria_mode\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, probe\u001b[39m.\u001b[39;49mfc\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdetach())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     artifacts\u001b[39m.\u001b[39mappend(artifact)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog_artifact(artifacts)\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/wandb/sdk/artifacts/artifact.py:1183\u001b[0m, in \u001b[0;36mArtifact.add_file\u001b[0;34m(self, local_path, name, is_tmp)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_can_add()\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(local_path):\n\u001b[0;32m-> 1183\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPath is not a file: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m local_path)\n\u001b[1;32m   1185\u001b[0m name \u001b[39m=\u001b[39m LogicalPath(name \u001b[39mor\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(local_path))\n\u001b[1;32m   1186\u001b[0m digest \u001b[39m=\u001b[39m md5_file_b64(local_path)\n",
      "\u001b[0;31mValueError\u001b[0m: Path is not a file: probe_A_starts.pt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f32a5b9ce50>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f329f317290, execution_count=153 error_before_exec=None error_in_exec=Path is not a file: probe_A_starts.pt info=<ExecutionInfo object at 7f329e295410, raw_cell=\"# import auto tqdm\n",
      "import wandb\n",
      "import torch\n",
      "impor..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# import auto tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "from src.probes import LinearProbe, MLPProbe\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "def all_probe_training_runner(\n",
    "    # data\n",
    "    embeddings,\n",
    "    vocab,\n",
    "    # task\n",
    "    alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",  # Task\n",
    "    criteria_mode=\"anywhere\",  # \"anywhere\", \"starting\" or \"posN\" (where N is a digit)\n",
    "    # arch\n",
    "    probe_type=\"linear\",\n",
    "    # hyperparameters\n",
    "    num_epochs=100,  # Define number of training epochs:\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    train_test_split=0.8,\n",
    "    rebalance=False,\n",
    "    # other config\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    use_wandb=False,\n",
    "):\n",
    "    group_name = wandb.util.generate_id() + \"_\" + alphabet\n",
    "\n",
    "    # Initialize an empty dictionary to store the learned weights for all letters (or, equivalently, 26 \"directions\", one for each linear probe)\n",
    "    embeddings_dim = embeddings.shape[1]\n",
    "    probes = {letter: LinearProbe(embeddings_dim).to(device) for letter in alphabet}\n",
    "\n",
    "    # Now loop over the alphabet and train/validate a probe for each letter:\n",
    "\n",
    "    for i, letter in enumerate(alphabet):\n",
    "        model = probes[letter]\n",
    "        nn.init.xavier_uniform_(model.fc.weight)  # best way to do this?\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n",
    "        )\n",
    "\n",
    "        if use_wandb:\n",
    "            config = {\n",
    "                \"letter\": letter,\n",
    "                \"criteria_mode\": criteria_mode,\n",
    "                \"model_name\": \"gpt-j\",\n",
    "                \"probe_type\": probe_type,\n",
    "                \"train_test_split\": train_test_split,\n",
    "                \"rebalance\": rebalance,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"device\": device,\n",
    "            }\n",
    "\n",
    "            wandb.init(\n",
    "                project=\"letter_presence_probes\",\n",
    "                group=group_name,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "        train_loader, test_loader = get_letter_dataset(\n",
    "            criterion=criteria_mode,\n",
    "            target=letter,\n",
    "            embeddings=embeddings,\n",
    "            vocab=vocab,\n",
    "            batch_size=batch_size,\n",
    "            rebalance=rebalance,\n",
    "            test_proportion=1 - train_test_split,\n",
    "        )\n",
    "\n",
    "        # Train the probe for the current letter:\n",
    "        model, other_artifacts = train_letter_probe_runner(\n",
    "            model=probes[letter],\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device,\n",
    "            use_wandb=use_wandb,\n",
    "        )\n",
    "        \n",
    "        probes[letter] = model\n",
    "\n",
    "    if use_wandb:\n",
    "        # Start a new W&B run for logging the aggregate artifact\n",
    "        wandb.init(project=\"letter_presence_probes\", name=\"aggregate_artifact_logging\")\n",
    "        # create_and_log_artifact(\n",
    "        #     probes,\n",
    "        #     \"all_probe_weights\",\n",
    "        #     \"model_tensors\",\n",
    "        #     \"All case-insensitive letter presence probe weights tensor\",\n",
    "        # )\n",
    "        \n",
    "        # log all probe weights\n",
    "        artifacts = []\n",
    "        for letter, probe in probes.items():\n",
    "            artifact = wandb.Artifact(\n",
    "                letter,\n",
    "                type=\"model_tensors\",\n",
    "                description=f\"Letter presence probe weights tensor for letter {letter}\",\n",
    "            )\n",
    "            artifact.add_file(f\"probe_{letter}_{criteria_mode}.pt\", probe.fc.weight.detach())\n",
    "            artifacts.append(artifact)\n",
    "        \n",
    "        wandb.log_artifact(artifacts)\n",
    "        \n",
    "        wandb.finish()  # End the run\n",
    "\n",
    "    return probes\n",
    "\n",
    "\n",
    "def train_letter_probe_runner(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    use_wandb=False,\n",
    "):\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    steps = 0\n",
    "    metric_frequency = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        pbar = tqdm(train_loader)\n",
    "\n",
    "        for batch_embeddings, batch_labels in pbar:\n",
    "            # Move your data to the chosen device during the training loop and ensure they're float32\n",
    "            # By explicitly converting to float32, you ensure that the data being fed into your model has the expected data type, and this should resolve the error you en\n",
    "            # batch_embeddings = batch_embeddings\n",
    "            # batch_labels = batch_labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings).squeeze()\n",
    "            loss = loss_fn(outputs, batch_labels.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if steps % metric_frequency == 0:\n",
    "                with torch.no_grad():\n",
    "                    probs = torch.sigmoid(outputs).detach() > 0.5\n",
    "                    \n",
    "                    accuracy = torchmetrics.functional.accuracy(probs, batch_labels, task = 'binary', num_classes=2).item()\n",
    "                    precision = torchmetrics.functional.precision( probs, batch_labels, task= 'binary', num_classes=2).item()\n",
    "                    recall = torchmetrics.functional.recall(probs, batch_labels, 'binary', num_classes=2).item()\n",
    "                    f1 = torchmetrics.functional.f1_score(probs, batch_labels, 'binary', num_classes=2).item()\n",
    "        \n",
    "                    # predictions\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch {epoch+1} / {num_epochs} | Loss: {loss:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\"\n",
    "                    )\n",
    "                    if use_wandb:\n",
    "                        # Log metrics to W&B\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"training_metrics/precision\": precision,\n",
    "                                \"training_metrics/recall\": recall,\n",
    "                                \"training_metrics/loss\": loss.item(),\n",
    "                                # \"training_metrics/f1_score\": f1_score,\n",
    "                                \"metrics/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                            },\n",
    "                            step=steps,\n",
    "                        )\n",
    "\n",
    "            steps += 1\n",
    "            \n",
    "        # EVALUATION (VALIDATION) PHASE\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Keep track of correct predictions and total predictions\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():  # Ensure no gradients are computed during validation\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_predictions = []  # Store all model predictions\n",
    "\n",
    "            for batch_embeddings, batch_labels in test_loader:\n",
    "                batch_embeddings = batch_embeddings.to(\n",
    "                    device\n",
    "                ).float()  # Ensure embeddings are on the correct device and dtype\n",
    "                batch_labels = batch_labels.to(\n",
    "                    device\n",
    "                ).float()  # Ensure labels are on the correct device and dtype\n",
    "\n",
    "                outputs = model(batch_embeddings).squeeze()\n",
    "\n",
    "                # Calculate loss on validation data\n",
    "                loss = loss_fn(outputs, batch_labels)\n",
    "                validation_loss += loss.item()  # Update validation loss\n",
    "\n",
    "                # Convert outputs to probabilities\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                predictions = (probs > 0.5).float()\n",
    "\n",
    "                # Update correct and total predictions\n",
    "                correct_preds += (predictions == batch_labels).sum().item()\n",
    "                total_preds += batch_labels.size(0)\n",
    "\n",
    "                # Append batch labels and predictions to all_labels and all_predictions\n",
    "                all_labels.append(batch_labels.cpu().numpy())\n",
    "                all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "            # Flatten all_labels and all_predictions lists and convert to numpy arrays\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "            # Compute F1 Score\n",
    "            f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "            validation_loss /= len(test_loader)  # Get the average validation loss\n",
    "\n",
    "            # Calculate accuracy and average loss\n",
    "            accuracy = correct_preds / total_preds\n",
    "            precision = precision_score(all_labels, all_predictions)\n",
    "            recall = recall_score(all_labels, all_predictions)\n",
    "\n",
    "            # print(f\"eval_metrics/accuracy: {accuracy * 100:.2f}%\")\n",
    "            # print(f\"eval_metrics/precision: {precision * 100:.2f}%\")\n",
    "            # print(f\"eval_metrics/recall: {recall * 100:.2f}%\")\n",
    "            # print(f\"eval_metrics/loss: {validation_loss:.4f}\")\n",
    "            # print(f\"eval_metrics/f1: {f1:.4f}\\n\")\n",
    "            scheduler.step(validation_loss)\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"eval_metrics/accuracy\": accuracy * 100,\n",
    "                        \"eval_metrics/precision\": precision * 100,\n",
    "                        \"eval_metrics/recall\": recall * 100,\n",
    "                        \"eval_metrics/loss\": validation_loss,\n",
    "                        \"eval_metrics/f1\": f1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    return model, 0\n",
    "\n",
    "probe_weights_tensor = all_probe_training_runner(\n",
    "    embeddings=model.W_E.detach(),\n",
    "    vocab=vocab,\n",
    "    # alphabet=\"A\",\n",
    "    criteria_mode=\"starts\",\n",
    "    probe_type=\"linear\",\n",
    "    num_epochs=30,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.005,\n",
    "    train_test_split=0.95,\n",
    "    rebalance=True,\n",
    "    use_wandb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W_E.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/730 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/730 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 298, in __getitem__\n    return self.dataset[self.indices[idx]]\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_30633/236507202.py\", line 40, in __getitem__\n    self.negative_indices[idx -  self.n_positive],\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m pr\u001b[39m.\u001b[39menable()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Call the function you wish to profile\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m profile_all_probe_training_runner()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Disable the profiler\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m pr\u001b[39m.\u001b[39mdisable()\n",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprofile_all_probe_training_runner\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     probe_weights_tensor \u001b[39m=\u001b[39m all_probe_training_runner(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         embeddings\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mW_E,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         alphabet\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         criteria_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstarts\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         probe_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         num_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.005\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         train_test_split\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         rebalance\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         use_wandb\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     )\n",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     train_loader, test_loader \u001b[39m=\u001b[39m get_letter_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m         criterion\u001b[39m=\u001b[39mcriteria_mode,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m         target\u001b[39m=\u001b[39mletter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m         num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39m# Train the probe for the current letter:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     model, other_artifacts \u001b[39m=\u001b[39m train_letter_probe_runner(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m         model\u001b[39m=\u001b[39;49mprobes[letter],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m         train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         test_loader\u001b[39m=\u001b[39;49mtest_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m         num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m         use_wandb\u001b[39m=\u001b[39;49muse_wandb,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m use_wandb:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     \u001b[39m# Start a new W&B run for logging the aggregate artifact\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mletter_presence_probes\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maggregate_artifact_logging\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/root/github_repositories/alphabetical_probe/joseph.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()  \u001b[39m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(train_loader)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m (idx, batch_embeddings), batch_labels \u001b[39min\u001b[39;49;00m pbar:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m     \u001b[39m# Move your data to the chosen device during the training loop and ensure they're float32\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m     \u001b[39m# By explicitly converting to float32, you ensure that the data being fed into your model has the expected data type, and this should resolve the error you en\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m     \u001b[39m# batch_embeddings = batch_embeddings\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m     \u001b[39m# batch_labels = batch_labels.to(device).float()\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m     outputs \u001b[39m=\u001b[39;49m model(batch_embeddings)\u001b[39m.\u001b[39;49msqueeze()\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;49;00m obj \u001b[39min\u001b[39;49;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;49;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/conda/envs/alphabetical_probe/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 298, in __getitem__\n    return self.dataset[self.indices[idx]]\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_30633/236507202.py\", line 40, in __getitem__\n    self.negative_indices[idx -  self.n_positive],\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "import wandb\n",
    "\n",
    "import os \n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"True\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "# Wrap your function call in another function for cProfile to hook into\n",
    "def profile_all_probe_training_runner():\n",
    "    probe_weights_tensor = all_probe_training_runner(\n",
    "        embeddings=model.W_E,\n",
    "        vocab=vocab,\n",
    "        alphabet=\"A\",\n",
    "        criteria_mode=\"starts\",\n",
    "        probe_type=\"linear\",\n",
    "        num_epochs=1,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.005,\n",
    "        train_test_split=0.9,\n",
    "        num_workers=16,\n",
    "        rebalance=True,\n",
    "        use_wandb=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a Profile object\n",
    "pr = cProfile.Profile()\n",
    "# Enable the profiler\n",
    "pr.enable()\n",
    "\n",
    "# Call the function you wish to profile\n",
    "profile_all_probe_training_runner()\n",
    "\n",
    "# Disable the profiler\n",
    "pr.disable()\n",
    "\n",
    "# Create a StringIO object to output your profile results\n",
    "s = StringIO()\n",
    "\n",
    "# Sort the stats by cumulative time taken and print to stdout\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(\"cumulative\")\n",
    "ps.print_stats()\n",
    "\n",
    "# Print the result of the profile to the console\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24021, 31554, 24035, 23773, 29095,  2758,  7731, 42585,  7537, 19408,\n",
      "         6882, 43406, 49145, 45661, 16784, 25509])\n",
      "torch.Size([16, 4096])\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
      "tensor(0.5000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "<b>%{y}</b><br><br>%{x}",
         "legendgroup": "False",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "False",
         "offsetgroup": "False",
         "orientation": "h",
         "showlegend": true,
         "text": [
          "hus",
          "etz",
          "Detail",
          "eighteen",
          "Lean",
          "amus",
          "Rod",
          "Did"
         ],
         "textposition": "auto",
         "texttemplate": "%{x:0.2f}",
         "type": "bar",
         "x": [
          -0.14216502010822296,
          -0.12922854721546173,
          -0.06930471211671829,
          -0.042964402586221695,
          -0.008811245672404766,
          -0.004396491684019566,
          -0.004281620495021343,
          0.004590530879795551
         ],
         "xaxis": "x",
         "y": [
          "hus",
          "etz",
          "Detail",
          "eighteen",
          "Lean",
          "amus",
          "Rod",
          "Did"
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "<b>%{y}</b><br><br>%{x}",
         "legendgroup": "True",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "True",
         "offsetgroup": "True",
         "orientation": "h",
         "showlegend": true,
         "text": [
          "April",
          "Along",
          "Apr",
          "Agility",
          "Arrow",
          "Amtrak",
          "Ax",
          "Arc"
         ],
         "textposition": "auto",
         "texttemplate": "%{x:0.2f}",
         "type": "bar",
         "x": [
          0.05574823543429375,
          0.0859089344739914,
          0.15930087864398956,
          0.17775551974773407,
          0.1950386017560959,
          0.19846026599407196,
          0.22487865388393402,
          0.25492745637893677
         ],
         "xaxis": "x",
         "y": [
          "April",
          "Along",
          "Apr",
          "Agility",
          "Arrow",
          "Amtrak",
          "Ax",
          "Arc"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "height": 800,
        "legend": {
         "title": {
          "text": "Starts with A"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Probe weights for letter A"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "predictions"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "words"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ef330333-d409-4c0f-9091-c36f9c359478\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ef330333-d409-4c0f-9091-c36f9c359478\")) {                    Plotly.newPlot(                        \"ef330333-d409-4c0f-9091-c36f9c359478\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"\\u003cb\\u003e%{y}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e%{x}\",\"legendgroup\":\"False\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"False\",\"offsetgroup\":\"False\",\"orientation\":\"h\",\"showlegend\":true,\"text\":[\"hus\",\"etz\",\"Detail\",\"eighteen\",\"Lean\",\"amus\",\"Rod\",\"Did\"],\"textposition\":\"auto\",\"texttemplate\":\"%{x:0.2f}\",\"x\":[-0.14216502010822296,-0.12922854721546173,-0.06930471211671829,-0.042964402586221695,-0.008811245672404766,-0.004396491684019566,-0.004281620495021343,0.004590530879795551],\"xaxis\":\"x\",\"y\":[\"hus\",\"etz\",\"Detail\",\"eighteen\",\"Lean\",\"amus\",\"Rod\",\"Did\"],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"\\u003cb\\u003e%{y}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e%{x}\",\"legendgroup\":\"True\",\"marker\":{\"color\":\"#EF553B\",\"pattern\":{\"shape\":\"\"}},\"name\":\"True\",\"offsetgroup\":\"True\",\"orientation\":\"h\",\"showlegend\":true,\"text\":[\"April\",\"Along\",\"Apr\",\"Agility\",\"Arrow\",\"Amtrak\",\"Ax\",\"Arc\"],\"textposition\":\"auto\",\"texttemplate\":\"%{x:0.2f}\",\"x\":[0.05574823543429375,0.0859089344739914,0.15930087864398956,0.17775551974773407,0.1950386017560959,0.19846026599407196,0.22487865388393402,0.25492745637893677],\"xaxis\":\"x\",\"y\":[\"April\",\"Along\",\"Apr\",\"Agility\",\"Arrow\",\"Amtrak\",\"Ax\",\"Arc\"],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"predictions\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"words\"}},\"legend\":{\"title\":{\"text\":\"Starts with A\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Probe weights for letter A\"},\"barmode\":\"relative\",\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ef330333-d409-4c0f-9091-c36f9c359478');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, test_loader = get_letter_dataset(\n",
    "    criterion=\"starts\",\n",
    "    target=\"A\",\n",
    "    embeddings=model.W_E,\n",
    "    vocab=vocab,\n",
    "    batch_size=16,\n",
    "    rebalance=True,\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch[0][0])\n",
    "    print(batch[0][1].shape)\n",
    "    print(batch[1])\n",
    "    # get proportions of positive and negative labels\n",
    "    print(sum(batch[1]) / len(batch[1]))\n",
    "    break\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_batch_predictions(words, embeddings, probe):\n",
    "    predictions = probe(embeddings).squeeze()\n",
    "    predictions = predictions.squeeze()\n",
    "\n",
    "    # make a dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"words\": words,\n",
    "            \"predictions\": predictions.cpu().detach().numpy(),\n",
    "            \"labels\": batch[1].cpu().detach().numpy(),\n",
    "        }\n",
    "    )\n",
    "    # convert labels to bool\n",
    "    df[\"labels\"] = df[\"labels\"].astype(bool)\n",
    "    # sort by predictions\n",
    "    df = df.sort_values(by=[\"predictions\"], ascending=True)\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        y=\"words\",\n",
    "        x=\"predictions\",\n",
    "        color=\"labels\",\n",
    "        title=\"Probe weights for letter A\",\n",
    "        labels={\n",
    "            \"x\": \"Word\",\n",
    "            \"y\": \"Probe Logit\",\n",
    "            \"labels\": \"Starts with A\",\n",
    "        },\n",
    "        text_auto=\"0.2f\",\n",
    "        text=\"words\",\n",
    "        height=800,\n",
    "        # width=1200,\n",
    "        template=\"plotly_white\",\n",
    "        category_orders={\"index\": df.index[::-1]},\n",
    "    )\n",
    "    # update the hover template to make the word much larger\n",
    "    fig.update_traces(hovertemplate=\"<b>%{y}</b><br><br>%{x}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "words = [reverse_vocab[i.item()].strip(\"Ġ\") for i in batch[0][0]]\n",
    "plot_batch_predictions(words, batch[0][1], probe_weights_tensor[\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_weights_tensor[\"A\"].fc.weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f9fedb359d0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f9fedb01050, raw_cell=\"import wandb\n",
      "\n",
      "# shut down any sessions\n",
      "wandb.finis..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f9fedb359d0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f9fa0535790, execution_count=33 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f9fedb01050, raw_cell=\"import wandb\n",
      "\n",
      "# shut down any sessions\n",
      "wandb.finis..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22566173744149227d/root/github_repositories/alphabetical_probe/joseph.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# shut down any sessions\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding Up Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25950\n",
      "25950\n",
      "torch.Size([25950, 4096])\n",
      "25950\n"
     ]
    }
   ],
   "source": [
    "# Filter Step\n",
    "roman_char_regex = re.compile('^[A-Za-z]+$') # A to Z and a to z are Roman characters\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "# filter vocab to only include valid words\n",
    "\n",
    "\n",
    "def filter_vocab(vocab):\n",
    "    english_words = set(w.lower() for w in words.words())\n",
    "    new_vocab = {}\n",
    "    for word, index in vocab.items():\n",
    "        clean_word = word.lstrip(\"Ġ\").strip().lower()\n",
    "        if clean_word in english_words and bool(roman_char_regex.match(clean_word)):\n",
    "            new_vocab[word] = index\n",
    "    return new_vocab\n",
    "\n",
    "new_vocab = filter_vocab(vocab)\n",
    "print(len(new_vocab))\n",
    "\n",
    "vocab_tokens = list(new_vocab.keys())\n",
    "original_indices = list(new_vocab.values())\n",
    "index_mapping = {v: i for i, (k, v) in enumerate(new_vocab.items())}\n",
    "print(len(index_mapping))\n",
    "\n",
    "new_embeddings = model.W_E[list(new_vocab.values())]\n",
    "print(new_embeddings.shape)\n",
    "\n",
    "\n",
    "# word = \"almost\"\n",
    "# idx = vocab[word]\n",
    "# torch.testing.assert_close(model.W_E[idx], new_embeddings[index_mapping[idx]])\n",
    "\n",
    "# Label Steps\n",
    "\n",
    "regex_pattern = get_regex_pattern(\"A\", \"starts\")\n",
    "labels = [bool(re.search(regex_pattern, k.strip().strip(\"Ġ\"))) for k in vocab_tokens]\n",
    "labels = torch.tensor(labels, dtype=torch.bool)\n",
    "labels[:10]\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Step \n",
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(new_embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_size = int((1 - test_proportion) * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data loaders for the train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=make_weights_for_balanced_classes,\n",
    ")\n",
    "rebalance = True\n",
    "# rebalance\n",
    "if rebalance:\n",
    "    n_total = len(labels)\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    weight_per_class = {1: n_total / n_pos, 0: n_total / n_neg}\n",
    "    weights = [weight_per_class[1]] * n_pos + [weight_per_class[0]] * n_neg\n",
    "    train_weights = [weights[i] for i in train_dataset.indices]\n",
    "    sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=0)\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25950"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tensors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-3.2390e-02,  6.7799e-03,  1.5451e-02,  ...,  8.8690e-03,\n",
      "        -1.0720e-02, -7.4208e-05], device='cuda:0', grad_fn=<SelectBackward0>), tensor(False))\n",
      "0.001961231231689453\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(new_embeddings, labels)\n",
    "start = time.time()\n",
    "\n",
    "print(dataset[0])\n",
    "    \n",
    "end = time.time() - start\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "def create_and_log_artifact(tensor, name, artifact_type, description):\n",
    "    # Use a temporary file to save the tensor\n",
    "    with tempfile.NamedTemporaryFile(delete=True, suffix=\".pt\") as tmp:\n",
    "        torch.save(tensor, tmp.name)\n",
    "\n",
    "        # Create a new artifact\n",
    "        artifact = wandb.Artifact(\n",
    "            name=name,\n",
    "            type=artifact_type,\n",
    "            description=description,\n",
    "        )\n",
    "        artifact.add_file(tmp.name)\n",
    "\n",
    "        # Log the artifact\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "hidden_dim = 4096\n",
    "num_hidden_layers = 2\n",
    "\n",
    "\n",
    "def get_mlp_weights(model):\n",
    "    # Extracts weights from a trained MLP model and returns them as a dictionary.\n",
    "    #\n",
    "    #:param model: The trained model.\n",
    "    #:return: A dictionary containing weights and biases for each layer.\n",
    "\n",
    "    weights = {}\n",
    "    weights[\"input_weight\"] = model.fc_input.weight.data.clone().detach()\n",
    "    weights[\"input_bias\"] = model.fc_input.bias.data.clone().detach()\n",
    "\n",
    "    for idx, layer in enumerate(model.fc_hidden):\n",
    "        weights[f\"hidden_{idx+1}_weight\"] = layer.weight.data.clone().detach()\n",
    "        weights[f\"hidden_{idx+1}_bias\"] = layer.bias.data.clone().detach()\n",
    "\n",
    "    weights[\"output_weight\"] = model.fc_output.weight.data.clone().detach()\n",
    "    weights[\"output_bias\"] = model.fc_output.bias.data.clone().detach()\n",
    "\n",
    "    return weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphabetical_probe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
