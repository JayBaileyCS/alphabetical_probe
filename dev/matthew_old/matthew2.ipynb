{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwatkins1970/alphabetical_probe/blob/main/matthew2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRRsyuKnCYD5",
        "outputId": "fa3ae6d7-fb45-41c4-a1f5-dcdd1636208d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "GD_PATH = '/content/Drive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(GD_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ONLY NEED TO DO THIS ONCE\n",
        "\n",
        "#!git clone https://github.com/mwatkins1970/alphabetical_probe"
      ],
      "metadata": {
        "id": "EGPQAcmeIsdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Drive/My Drive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe\n",
        "!git pull\n",
        "\n",
        "#RECLONE IF NECESSARY\n",
        "#!rm -rf /content/Drive/My Drive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe\n",
        "#%cd /content/Drive/My Drive/SpellingMiracleCollab/ColabRepoCloning\n",
        "#!git clone https://github.com/mwatkins1970/alphabetical_probe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEMprbAJ-9nD",
        "outputId": "dae284ac-a1cc-47a8-eba3-b8e7adea7595"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Drive/My Drive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"drsyntax@hotmail.com\"\n",
        "!git config --global user.name \"mwatkins1970\""
      ],
      "metadata": {
        "id": "fofrJVCw-kJx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO PUSH\n",
        "\n",
        "%cd /content/Drive/My Drive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe\n",
        "\n",
        "!git remote set-url origin https://mwatkins1970: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@github.com/mwatkins1970/alphabetical_probe.git\n",
        "\n",
        "!git add -A\n",
        "!git commit -m \"Description of changes\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "HuezaN94-sqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5GLBnl4JFja",
        "outputId": "be489fe0-fa00-41fd-a644-5113ceda739d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdev\u001b[0m/           \u001b[01;34mmodels\u001b[0m/  README.md         \u001b[01;34msrc\u001b[0m/    \u001b[01;34mto_be_sorted\u001b[0m/  \u001b[01;34mweights\u001b[0m/\n",
            "matthew.ipynb  \u001b[01;34mmyenv\u001b[0m/   requirements.txt  \u001b[01;34mtests\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd src\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXRwH2k0JK4y",
        "outputId": "f401b534-f62c-4fb1-e13b-a5e04613e033"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Drive/MyDrive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe/src\n",
            "dataset.py                MLPs.py            probe_utils.py\n",
            "get_training_data.py      model_loading.py   \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            "__init__.py               \u001b[01;34mmodels\u001b[0m/            token_utils.py\n",
            "length_probe_training.py  probes.py\n",
            "letter_token_utils.py     probe_training.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python probes.py\n",
        "!python dataset.py\n",
        "!python MLPs.py"
      ],
      "metadata": {
        "id": "QYT592JzJWBZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "nmP3L4p9Kmjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08f9bc8-6bd1-4ec3-933f-84b118a66b0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/258.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/258.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: huggingface-hub, accelerate\n",
            "Successfully installed accelerate-0.23.0 huggingface-hub-0.17.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WHENEVER YOU EDIT ONE OF THESE .py MODULES HERE, DO THIS\n",
        "\n",
        "import importlib\n",
        "import get_prompt_based_starting_triple  # ensure you've imported it the first time\n",
        "\n",
        "importlib.reload(get_prompt_based_starting_triple)"
      ],
      "metadata": {
        "id": "EY6Qkw57BIV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.29.2  #an old version which supports my gpt-j code\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/alphabetical_probe')  # Adjust the path accordingly\n",
        "\n",
        "import model_loading\n",
        "\n",
        "tokenizer, GPTmodel, embeddings = model_loading.load_or_download_model_tok_emb()"
      ],
      "metadata": {
        "id": "J06eKYONKWSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230bb488-cf96-4afe-87d7-a29bf7145abc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.2\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5.1/7.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2023.7.22)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n",
            "Loading EleutherAI/gpt-j-6B tokenizer from local storage...\n",
            "Loading EleutherAI/gpt-j-6B model from local storage...\n",
            "Loading EleutherAI/gpt-j-6B embeddings from local storage...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a bunch of stuff that got lost when I cleared my cache. Annoying, but I can reconstruct it. The next cell was an experiment in running first-letter probe training; there was a clone cell that followed and trained length probes;\n",
        "\n",
        "Get this running, then try to do it with wandb switched on; try upper/lower case versions\n"
      ],
      "metadata": {
        "id": "4RhPVevr9dSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SORT THIS MESS OUT\n",
        "\n",
        "# First, ensure the module can be imported by adding its path to sys.path\n",
        "import sys\n",
        "import token_utils\n",
        "\n",
        "sys.path.append(\"/content/Drive/MyDrive/SpellingMiracleCollab/ColabRepoCloning/alphabetical_probe\")\n",
        "\n",
        "token_utils.tokenizer = tokenizer\n",
        "token_strings, all_rom_token_indices = token_setup(tokenizer)\n",
        "\n",
        "# Now, import the necessary components from the module\n",
        "from probe_training import all_probe_training_runner\n",
        "\n",
        "# Call the function with the necessary parameters\n",
        "probe_weights_tensor = all_probe_training_runner(embeddings, all_rom_token_indices, token_strings)\n",
        "print(probe_weights_tensor)"
      ],
      "metadata": {
        "id": "m_TxGkDhANfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I DID WANT TO MOVE MY SPELL EXPERIMENT INTO HERE, BUT MAY LEAVE IT.\n",
        "WE'RE WORKING ON LETTER PRESENCE PROBES, PRIMARILY.\n",
        "\n",
        "WOULD BE GOOD TO INTEGRATE IT WITH THIS FORMAT THO'\n",
        "\n"
      ],
      "metadata": {
        "id": "8ii0OSu6AyNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# THIS MAKES A LIST OF THE 50257 SIGNIFICANT TOKENS, AS WELL AS  SUB-LISTS OF ALL-ROMAN TOKENS\n",
        "\n",
        "# Define the file path for the token_strings list\n",
        "file_path = \"/content/Drive/My Drive/SpellingMiracleCollab/token_strings.pkl\"\n",
        "\n",
        "if os.path.exists(file_path):     # if it's already saved, load it\n",
        "  with open(file_path, 'rb') as file:\n",
        "      token_strings = pickle.load(file)\n",
        "else:                             # otherwise create it and sav it.\n",
        "  token_strings = [tokenizer.decode([i]) for i in range(50257)]\n",
        "  with open(file_path, 'wb') as file:\n",
        "      pickle.dump(token_strings, file)\n",
        "\n",
        "num_tokens = len(token_strings)\n",
        "\n",
        "# The leading spaces in these token strings are weird, they somehow delete themselves and the character before whatever they get appended to\n",
        "#...so you got stuff like \"The string'petertodd' spelled in all capital letters...\"\n",
        "#...rather than \"The string ' petertodd' spelled is...\" as it should be\n",
        "# This seems to be an easy fix. Just go through the lists and replace the first character with an actual space!\n",
        "for token in token_strings:\n",
        "  token = \" \" + token[1:]\n",
        "\n",
        "print(f\"There are {num_tokens} tokens.\")\n",
        "\n",
        "all_rom_tokens = []\t\t\t#initialise list of all-roman tokens\n",
        "all_rom_token_indices = []\n",
        "\n",
        "for i in range(num_tokens):\n",
        "\tall_rom = True                       # Is token_string[i] all roman characters? Assume to begin that it is.\n",
        "\tfor ch in range(len(token_strings[i])):\n",
        "\t\tif token_strings[i][ch] not in ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
        "\t\t\tall_rom = False\n",
        "\tif all_rom == True and token_strings[i] not in [' ', '  ', '   ', '    ', '     ', '      ', '       ', '        ']:\n",
        "\t\tall_rom_tokens.append(token_strings[i])\n",
        "\t\tall_rom_token_indices.append(i)\n",
        "\n",
        "all_rom_token_gt2_indices = [idx for idx in all_rom_token_indices if len(token_strings[idx].lstrip()) > 2]\n",
        "\n",
        "\n",
        "print(f\"There are {len(all_rom_tokens)} all-Roman tokens.\")\n",
        "print(f\"There are {len(all_rom_token_gt2_indices)} all-Roman tokens with more than two letters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtBP9RzeA5a8",
        "outputId": "bd8a792b-4a9e-4281-93ad-4ea90723f011"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 50257 tokens.\n",
            "There are 46893 all-Roman tokens.\n",
            "There are 44634 all-Roman tokens with more than two letters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_probe(embedding_vector, probes_tensor):    # HERE probes_tensor should be shape-(26,4096), with rows corresponding to alphabet\n",
        "    # Check if all rows are zeros\n",
        "    if torch.all(probes_tensor == 0):\n",
        "        return '_'\n",
        "\n",
        "    # Create a mask to identify non-zero rows in probes_tensor\n",
        "    non_zero_mask = torch.any(probes_tensor != 0, dim=1)\n",
        "\n",
        "    # Filter out zero rows from probes_tensor\n",
        "    filtered_probes_tensor = probes_tensor[non_zero_mask]\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = F.cosine_similarity(embedding_vector.unsqueeze(0), filtered_probes_tensor)\n",
        "\n",
        "    # Get the closest value and index\n",
        "    top_value, top_index = torch.topk(cosine_sim, 1)\n",
        "\n",
        "    # Convert the filtered tensor index to the original tensor index\n",
        "    original_index = torch.arange(probes_tensor.size(0))[non_zero_mask][top_index]\n",
        "\n",
        "    # Convert index to letter and build result list\n",
        "    closest_probe = chr(original_index.item() + ord('a'))\n",
        "\n",
        "    return closest_probe\n"
      ],
      "metadata": {
        "id": "dgUEUjRfDZCx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_data(target_triple, num_samples, embeddings, all_rom_token_gt2_indices):\n",
        "\t# Fetch indices for tokens that begin with the specified adjacent-letter-triple\n",
        "\tpositive_indices = [index for index in all_rom_token_gt2_indices if token_strings[index].lstrip().lower()[0:3] == target_triple]\n",
        "\n",
        "\t# Fetch indices for tokens that do not begin this way\n",
        "\t# (by taking a set difference and then converting back to a list)\n",
        "\tnegative_indices = list(set(all_rom_token_gt2_indices) - set(positive_indices))\n",
        "\n",
        "\t# Randomly sample from positive and negative indices to balance the dataset\n",
        "\tnum_positive = min(num_samples // 2, len(positive_indices))\n",
        "\tnum_negative = num_samples - num_positive\n",
        "\n",
        "\tsampled_positive_indices = random.sample(positive_indices, num_positive)\n",
        "\tsampled_negative_indices = random.sample(negative_indices, num_negative)\n",
        "\n",
        "\t# Combine sampled indices\n",
        "\tsampled_indices = sampled_positive_indices + sampled_negative_indices\n",
        "\trandom.shuffle(sampled_indices)  # Shuffle combined indices for randomness in training\n",
        "\n",
        "\t# Extract corresponding embeddings and labels\n",
        "\tall_embeddings = embeddings[sampled_indices]\n",
        "\tall_labels = [1 if idx in positive_indices else 0 for idx in sampled_indices]\n",
        "\n",
        "\treturn all_embeddings.clone().detach(), torch.tensor(all_labels).clone().detach()\n",
        ""
      ],
      "metadata": {
        "id": "0t0J7_9cEJub"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def starting_triple_probe_train(target_triple):  # THIS FUNCTION TRAINS A PROBE TO CLASSIFY TOKENS ACCORDING TO FIRST THREE LETTERS = target_triple\n",
        "\n",
        "  print(f\"Starting triple: {target_triple}\")\n",
        "\n",
        "  # construct tensors of embeddings and labels for training and validation\n",
        "  all_embeddings, all_labels = get_training_data(target_triple, num_samples, embeddings, all_rom_token_gt2_indices)\n",
        "\n",
        "  print(f\"There are {sum([1 for idx in all_rom_token_gt2_indices if token_strings[idx].lstrip()[0:3].lower() == target_triple])} tokens starting with this triple.\")\n",
        "\n",
        "  if sum([1 for idx in all_rom_token_gt2_indices if token_strings[idx].lstrip()[0:3].lower() == target_triple]) > 15:\n",
        "    # arbitrary number of tokens with correct starting letters to act as minimum cutoff - PROBABLY FAR TOO LOW?\n",
        "\n",
        "    # split the data into training and validation sets (using a function from the sklearn.model_selection module)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(all_embeddings, all_labels, test_size=0.2, random_state=42, stratify=all_labels)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = LinearProbe(4096).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "    criterion = nn.BCEWithLogitsLoss()         # Binary cross-entropy loss with logits (because we haven't used an activation in our model)\n",
        "        \t                            # This combines sigmoid activation, which converts logits to probabilities, and binary cross entropy loss\n",
        "              \t        # outputs will be probabilities 0 < p < 1 that the letter belongs to the token The label will be 0 or 1 (it doesn't or it does)\n",
        "\n",
        "    # create DataLoader for your training dataset\n",
        "    train_dataset = LetterDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    #X_train, y_train (embeddings and labels for training) were created above using standard methods applied to all_embeddings and all_labels tensors\n",
        "\n",
        "    # create DataLoader for your validation dataset\n",
        "    val_dataset = LetterDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    #X_val, y_val (embeddings and labels for validation) were likewise created above using standard methods applied to all_embeddings and all_labels tensors\n",
        "\n",
        "    # TRAINING LOOP\n",
        "\n",
        "    # initialise relevant variables for early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    no_improve_count = 0\n",
        "\n",
        "    print('\\n_________________________________________________\\n')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()  # Set the model to training mode\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for batch_embeddings, batch_labels in train_loader:\n",
        "        # Move your data to the chosen device during the training loop and ensure they're float32\n",
        "        # By explicitly converting to float32, you ensure that the data being fed into your model has the expected data type, and this should resolve the error you en\n",
        "        batch_embeddings = batch_embeddings.to(device).float()\n",
        "        batch_labels = batch_labels.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_embeddings).squeeze()\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"{target_triple}: epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # STORE THE PROBE WEIGHTS (or \"direction\" in embedding space associated with this probe)\n",
        "\n",
        "    letter_triple_probe_weights = model.fc.weight.data.clone().detach()\n",
        "\n",
        "\n",
        "    # EVALUATION (VALIDATION) PHASE\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create DataLoader for validation data\n",
        "    val_dataset = LetterDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "    # Keep track of correct predictions and total predictions\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    validation_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Ensure no gradients are computed during validation\n",
        "      for batch_embeddings, batch_labels in val_loader:\n",
        "        batch_embeddings = batch_embeddings.to(device).float()  # Ensure embeddings are on the correct device and dtype\n",
        "        batch_labels = batch_labels.to(device).float()  # Ensure labels are on the correct device and dtype\n",
        "\n",
        "        outputs = model(batch_embeddings).squeeze()\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        validation_loss += loss.item()\n",
        "\n",
        "        # Convert outputs to probabilities\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        predictions = (probs > 0.5).float()\n",
        "\n",
        "        # Update correct and total predictions\n",
        "        correct_preds += (predictions == batch_labels).sum().item()\n",
        "        total_preds += batch_labels.size(0)\n",
        "\n",
        "        # Early stopping and model checkpointing\n",
        "        if validation_loss < best_val_loss:\n",
        "          best_val_loss = validation_loss\n",
        "          best_train_loss = total_loss / len(train_loader)  # Store best training loss\n",
        "          torch.save(model.state_dict(), f\"model_{target_triple}.pth\")\n",
        "          no_improve_count = 0  # Reset counter\n",
        "        else:\n",
        "          no_improve_count += 1\n",
        "\n",
        "        if no_improve_count >= patience:\n",
        "          break\n",
        "\n",
        "    # Calculate accuracy and average loss\n",
        "    accuracy = correct_preds / total_preds\n",
        "    average_loss = validation_loss / len(val_loader)\n",
        "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Validation Loss: {average_loss:.4f}\")\n",
        "\n",
        "  else:   # if there's not enough data, write a row of zeros for this index\n",
        "    letter_triple_probe_weights = torch.zeros(4096)\n",
        "    #print(f\"Not enough tokens starting '{target_triple}', so writing zeros to this row of the (26,4096) tensor for the starting pair {target_triple[0:2]}.\")\n",
        "\n",
        "  return letter_triple_probe_weights\n"
      ],
      "metadata": {
        "id": "VgJpJdL4EJ4K"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probe_based_starting_triple(idx, weights_tensor):\n",
        "  print(\"Probe-based prediction:\")\n",
        "  # FIRST LETTER: EASY, LOOK FOR NEAREST FIRST-LETTER PROBE IN THE 26-ROW TENSOR - THIS WORKS REALLY WELL\n",
        "  pred_first_letter = find_closest_probe(embeddings[idx], weights_tensor)\n",
        "  print(f\"PREDICTED FIRST LETTER: {pred_first_letter.upper()}\")\n",
        "\n",
        "\n",
        "  # SECOND LETTER: ALMOST AS EASY, LOOK FOR THE NEAREST FIRST-LETTER PROBE IN THE APPROPRIATE 26-ROW TENSOR (GIVEN FRIST LETTER)\n",
        "\n",
        "  relevant_pair_probe_tensor = torch.load('/content/Drive/My Drive/SpellingMiracleCollab/pairs_starting_' + pred_first_letter + '_tensor.pt')\n",
        "\n",
        "  pred_second_letter = find_closest_probe(embeddings[idx], relevant_pair_probe_tensor)\n",
        "  print(f\"PREDICTED SECOND LETTER: {pred_second_letter.upper()}\")\n",
        "\n",
        "\n",
        "  # THIRD LETTER: TRICKIER, AS WE NOW NEED TO TRAIN 26 PROBES AND BUNDLE THEM INTO A 26-ROW TENSOR\n",
        "  # Now build starting_triple_probe_weights (a shape-(26,4096) tensor specific to the predicted starting pair)\n",
        "\n",
        "  if os.path.exists('/content/Drive/My Drive/SpellingMiracleCollab/triples_starting_' + pred_first_letter + pred_second_letter + '_tensor.pt'):\n",
        "      triple_probe_weights_tensor = torch.load('/content/Drive/My Drive/SpellingMiracleCollab/triples_starting_' + pred_first_letter + pred_second_letter + '_tensor.pt')\n",
        "  else:\n",
        "      print(\"\\nSorry, I just have to train up to 26 linear probes for the relevant three letter combinations (and then pick the closest one).\")\n",
        "      print(\"Might take a minute.\")\n",
        "      triple_probe_weights_tensor = torch.zeros(26,4096)\n",
        "\n",
        "      for k, char in enumerate('abcdefghijklmnopqrstuvwxyz'):\n",
        "\n",
        "        target_start_triple = pred_first_letter + pred_second_letter + char\n",
        "\n",
        "        triple_probe_weights_tensor[k] = starting_triple_probe_train(target_start_triple)\n",
        "        # SO THIS SHOULD SEQUENTIALLY WRITE IN THE 26 PROBE VECTOR/DIRECTIONS (A ROW OF ZEROS WHERE TRAINING ISN'T POSS.)\n",
        "\n",
        "      torch.save(triple_probe_weights_tensor, '/content/Drive/My Drive/SpellingMiracleCollab/triples_starting_' + pred_first_letter + pred_second_letter + '_tensor.pt')\n",
        "      #SAVE TO AVOID DUPLICATING EFFORT\n",
        "\n",
        "  pred_third_letter = find_closest_probe(embeddings[idx], triple_probe_weights_tensor)\n",
        "  print(f\"PREDICTED THIRD LETTER: {pred_third_letter.upper()}\")\n",
        "\n",
        "  return pred_first_letter + pred_second_letter + pred_third_letter\n"
      ],
      "metadata": {
        "id": "BtwgApQQEJ-S"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_based_starting_triple(idx):\n",
        "\n",
        "        token = token_strings[idx]\n",
        "        print(\"Prompt-based prediction:\")\n",
        "        #prompt1 = f'The string \"{token}\" begins with the letter \"'\n",
        "        prompt1 = f'The string \"house\", spelled in all capital letters separated by hyphens, looks like this: H-O-U-S-E\\nThe string \" Through\", spelled in all capital letters separated by hyphens, looks like this: T-H-R-O-U-G-H\\nThe string \"{token}\", spelled in all capital letters separated by hyphens, looks like this: '\n",
        "\n",
        "        ix1 = tokenizer.encode(prompt1)\n",
        "\n",
        "        model_out = GPTmodel.generate(\n",
        "                torch.tensor(ix1).unsqueeze(0).to(device),\n",
        "                max_length=len(ix1) + 1,\n",
        "                temperature=0.00000000001,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        output = tokenizer.decode(model_out[0])\n",
        "\n",
        "        pred_first_letter = output[-1].upper()\n",
        "\n",
        "        print(f\"PREDICTED FIRST LETTER: {pred_first_letter}\")\n",
        "\n",
        "        prompt2 = f'The string \"house\", spelled in all capital letters separated by hyphens, looks like this: H-O-U-S-E\\nThe string \" Through\", spelled in all capital letters separated by hyphens, looks like this: T-H-R-O-U-G-H\\nThe string \"{token}\", spelled in all capital letters separated by hyphens, looks like this: {pred_first_letter.upper()}-'\n",
        "        #prompt2 = f'The string \"{token}\", spelled in all capital letters separated by hyphens, looks like this: {pred_first_letter.upper()}-'\n",
        "        ix2 = tokenizer.encode(prompt2)\n",
        "\n",
        "        model_out = GPTmodel.generate(\n",
        "                torch.tensor(ix2).unsqueeze(0).to(device),\n",
        "                max_length= len(ix2) + 1,\n",
        "                temperature=0.00000000001,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        output = tokenizer.decode(model_out[0])\n",
        "\n",
        "        pred_second_letter = output[-1].upper()\n",
        "\n",
        "        print(f\"PREDICTED SECOND LETTER: {pred_second_letter}\")\n",
        "\n",
        "        prompt3 = f'The string \"house\", spelled in all capital letters separated by hyphens, looks like this: H-O-U-S-E\\nThe string \" Through\", spelled in all capital letters separated by hyphens, looks like this: T-H-R-O-U-G-H\\nThe string \"{token}\", spelled in all capital letters separated by hyphens, looks like this: {pred_first_letter.upper()}-{pred_second_letter.upper()}-'\n",
        "        #prompt3 = f'The string \"{token}\", spelled in all capital letters separated by hyphens, looks like this: {pred_first_letter.upper()}-{pred_second_letter.upper()}-'\n",
        "        ix3 = tokenizer.encode(prompt3)\n",
        "\n",
        "        model_out = GPTmodel.generate(\n",
        "                torch.tensor(ix3).unsqueeze(0).to(device),\n",
        "                max_length= len(ix3) + 1,\n",
        "                temperature=0.00000000001,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        output = tokenizer.decode(model_out[0])\n",
        "\n",
        "        pred_third_letter = output[-1].upper()\n",
        "\n",
        "        print(f\"PREDICTED THIRD LETTER: {pred_third_letter}\\n\")\n",
        "\n",
        "        return pred_first_letter + pred_second_letter + pred_third_letter\n"
      ],
      "metadata": {
        "id": "5H0fazIOEKEG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS SHOULD RUN THE EXPERIMENT *SET LAST TOKEN PROCESSED IN LINE 20*\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "probe_weights_tensor = torch.load('/content/Drive/My Drive/SpellingMiracleCollab/starting_letter_probe_weights_tensor.pt')\n",
        "\n",
        "GPTmodel = GPTmodel.to(torch.float32)\n",
        "\n",
        "# Define a 'patience' value for early stopping:\n",
        "patience = 10\n",
        "\n",
        "# Define number of samples in training+validation dataset:\n",
        "num_samples = 10000\n",
        "\n",
        "# Define number of training epochs:\n",
        "num_epochs = 100\n",
        "\n",
        "spell_record = []\n",
        "\n",
        "\n",
        "for idx in all_rom_token_gt2_indices[all_rom_token_gt2_indices.index(token_strings.index(' ruled'))+1:]:\n",
        "  token = token_strings[idx]\n",
        "  print(f\"TOKEN: '{token}'; INDEX: {idx}\")\n",
        "  prompt_triple = get_prompt_based_starting_triple(idx)\n",
        "  probe_triple = get_probe_based_starting_triple(idx, probe_weights_tensor)\n",
        "  actual_triple = token.lstrip().lower()[0:3]\n",
        "\n",
        "  if prompt_triple.lower() == actual_triple and probe_triple.lower() == actual_triple:\n",
        "    print(f\"\\nBOTH PROMPT- AND PROBE-BASED SPELLINGS CORRECT\\n\")\n",
        "  else:\n",
        "    print(f\"\\nPROMPT-BASED: {prompt_triple}; PROBE-BASED: {probe_triple.upper()}\\n\")\n",
        "  spell_record.append((idx, token, prompt_triple.upper(), prompt_triple.lower() == actual_triple, probe_triple.upper(), probe_triple.lower() == actual_triple))\n"
      ],
      "metadata": {
        "id": "aqGlKF04Ejrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}