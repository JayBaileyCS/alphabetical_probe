{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/miniforge3/envs/alphabet/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\t\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, utils, AutoTokenizer, GPTJForCausalLM\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# move workind directory to the root of the project\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt2 tokenizer...\n",
      "Downloading gpt2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type gptj. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of GPTJForCausalLM were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.attn.k_proj.weight', 'h.8.attn.k_proj.weight', 'h.3.mlp.fc_in.bias', 'h.1.mlp.fc_in.bias', 'h.3.attn.k_proj.weight', 'h.6.mlp.fc_out.bias', 'h.11.attn.k_proj.weight', 'h.3.mlp.fc_in.weight', 'h.10.attn.q_proj.weight', 'h.4.attn.q_proj.weight', 'h.6.attn.k_proj.weight', 'h.0.attn.k_proj.weight', 'h.3.mlp.fc_out.weight', 'h.7.mlp.fc_in.bias', 'h.3.mlp.fc_out.bias', 'h.5.attn.out_proj.weight', 'h.1.mlp.fc_in.weight', 'h.7.attn.out_proj.weight', 'lm_head.bias', 'h.2.mlp.fc_in.weight', 'h.0.mlp.fc_in.weight', 'h.4.attn.k_proj.weight', 'h.5.attn.v_proj.weight', 'h.0.mlp.fc_out.bias', 'h.0.attn.v_proj.weight', 'h.2.attn.k_proj.weight', 'h.0.attn.out_proj.weight', 'h.5.attn.k_proj.weight', 'h.1.attn.q_proj.weight', 'h.8.attn.out_proj.weight', 'h.1.attn.v_proj.weight', 'h.10.attn.k_proj.weight', 'h.2.attn.v_proj.weight', 'h.3.attn.v_proj.weight', 'h.5.mlp.fc_out.bias', 'h.9.mlp.fc_in.weight', 'h.9.mlp.fc_out.bias', 'h.1.mlp.fc_out.bias', 'h.6.mlp.fc_in.weight', 'h.0.mlp.fc_in.bias', 'h.10.mlp.fc_out.weight', 'h.2.mlp.fc_in.bias', 'h.11.mlp.fc_out.weight', 'h.8.mlp.fc_in.weight', 'h.0.mlp.fc_out.weight', 'h.11.attn.q_proj.weight', 'h.3.attn.out_proj.weight', 'h.11.mlp.fc_in.bias', 'h.9.attn.out_proj.weight', 'h.11.mlp.fc_in.weight', 'h.3.attn.q_proj.weight', 'h.4.attn.out_proj.weight', 'h.4.mlp.fc_in.weight', 'lm_head.weight', 'h.4.mlp.fc_in.bias', 'h.6.attn.v_proj.weight', 'h.1.attn.out_proj.weight', 'h.7.attn.v_proj.weight', 'h.2.mlp.fc_out.bias', 'h.11.mlp.fc_out.bias', 'h.11.attn.v_proj.weight', 'h.8.attn.q_proj.weight', 'h.5.attn.q_proj.weight', 'h.6.attn.out_proj.weight', 'h.5.mlp.fc_out.weight', 'h.4.attn.v_proj.weight', 'h.6.mlp.fc_in.bias', 'h.9.attn.k_proj.weight', 'h.1.attn.k_proj.weight', 'h.2.attn.out_proj.weight', 'h.11.attn.out_proj.weight', 'h.0.attn.q_proj.weight', 'h.4.mlp.fc_out.weight', 'h.9.attn.q_proj.weight', 'h.1.mlp.fc_out.weight', 'h.7.mlp.fc_out.weight', 'h.6.attn.q_proj.weight', 'h.10.mlp.fc_out.bias', 'h.10.mlp.fc_in.weight', 'h.10.attn.v_proj.weight', 'h.7.mlp.fc_out.bias', 'h.6.mlp.fc_out.weight', 'h.8.mlp.fc_out.weight', 'h.2.mlp.fc_out.weight', 'h.7.attn.q_proj.weight', 'h.7.mlp.fc_in.weight', 'h.8.mlp.fc_in.bias', 'h.8.mlp.fc_out.bias', 'h.5.mlp.fc_in.weight', 'h.9.mlp.fc_in.bias', 'h.10.attn.out_proj.weight', 'h.2.attn.q_proj.weight', 'h.9.attn.v_proj.weight', 'h.9.mlp.fc_out.weight', 'h.8.attn.v_proj.weight', 'h.4.mlp.fc_out.bias', 'h.5.mlp.fc_in.bias', 'h.10.mlp.fc_in.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gpt2 'embeddings' tensor has been saved.\n"
     ]
    }
   ],
   "source": [
    "def load_or_download_model(model_name=\"EleutherAI/gpt-j-6B\", device = \"cpu\"):\n",
    "    if not os.path.exists(f'./models/{model_name}'):\n",
    "        os.makedirs(f'./models/{model_name}', exist_ok=True)\n",
    "\n",
    "    TOKENIZER_PATH = f\"./models/{model_name}/tokenizer.pt\"\n",
    "    MODEL_PATH = f\"./models/{model_name}/model.pt\"\n",
    "    EMBEDDINGS_PATH = f\"./models/{model_name}/embeddings.pt\"\n",
    "\n",
    "    # Load or Download Tokenizer\n",
    "    if os.path.exists(TOKENIZER_PATH):\n",
    "        print(f'Loading {model_name} tokenizer from local storage...')\n",
    "        tokenizer = torch.load(TOKENIZER_PATH)\n",
    "    else:\n",
    "        print(f'Downloading {model_name} tokenizer...')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\")\n",
    "        torch.save(tokenizer, TOKENIZER_PATH)\n",
    "\n",
    "    # Load or Download Model\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f'Loading {model_name} model from local storage...')\n",
    "        GPTmodel = torch.load(MODEL_PATH).to(device)\n",
    "    else:\n",
    "        print(f'Downloading {model_name} model...')\n",
    "        GPTmodel = GPTJForCausalLM.from_pretrained(f\"{model_name}\").to(device)\n",
    "        torch.save(GPTmodel, MODEL_PATH)\n",
    "        \n",
    "    GPTmodel.eval()\n",
    "\n",
    "    # Save or Load Embeddings\n",
    "    if os.path.exists(EMBEDDINGS_PATH):\n",
    "        print(f'Loading {model_name} embeddings from local storage...')\n",
    "        embeddings = torch.load(EMBEDDINGS_PATH).to(device)\n",
    "    else:\n",
    "        embeddings = GPTmodel.transformer.wte.weight.to(device)\n",
    "        torch.save(embeddings, EMBEDDINGS_PATH)\n",
    "        print(f\"The {model_name} 'embeddings' tensor has been saved.\")\n",
    "\n",
    "    return tokenizer, GPTmodel, embeddings\n",
    "\n",
    "# Call the function with desired model name\n",
    "tokenizer, GPTmodel, embeddings = load_or_download_model(\n",
    "    model_name=\"gpt2\", device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spelling_collab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
